# X-former: Comparative Analysis of Transformer Models


X-former is a repository created with the goal of comparing the computational and memory efficiency of different transformer models or variants that have been published since the conception of the Transformer. The objective of this repository is to provide an organized and comprehensive overview of existing work and models across multiple domains.

## Motivation

The Transformer architecture, introduced by Vaswani et al. in 2017, has revolutionized the field of natural language processing and has gained significant attention in other domains as well. However, as Transformer models have evolved and grown in complexity, there is a need to analyzed and compare their computational and memory requirements. X-former aims to address this need by providing a platform for evaluating the efficiency of various transformer models. 

## Features

* Comparative analysis
* Efficiency metrics
* Organized Overview
* Domain-specific analyis
* Performance benchmarks

## Getting Started 

To get started with X-former, follow these steps: 

1. Clone the repository
2. Install the required dependencies
3. Explore the available transformer models and variants in the repository
4. Run the evaluation scripts to compare their computational and memory efficiency
5. Analyze the results and gain insights into the trade-offs between efficiency and performance

## Contributing

Contributions to X-former are welcome! If you would like to contribute to the repository, please follow these [guidelines](dhdj)

## License

X-former is released under the [MIT License](hdhlkhf). You are free to use, modify, and distribute the codebase in accordance with the terms of this license.
